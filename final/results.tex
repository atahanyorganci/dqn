\section{Results}
The agent was trained for 100000 episodes with random restarts every 20000 episodes. For evaluating the performance the agent average game length, mean \(Q\) value, mean reward, and mean loss is recorded. Results from one random restarts is shown in ~\autoref{fig:results} in~\autoref{app:results}. From the figures we can conclude that the agent improves its knowledge about the game nearly every episode. The graphs are not smoothed out, even tough the trajectory is clearly upwards for received reward, and as the agent explores the reward fluctuates. Lastly, the random agent scores \(1.5\) on average, and the best agent we trained scores 28 on average \(1886\%\) better than taking random actions.
