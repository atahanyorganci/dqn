\section{Conclusion}
All in all, we successfully implemented a deep reinforcement learning architecture using convolutional neural networks, trained with Deep Q-Learning, whose input is raw pixel values, varying between \( \{0, 1, \dots, 255\} \), of a grayscale version of the image of the current state, and output is the most suitable action from the action space. As we demonstrated in our final presentation demo, our trained agent outperforms the random agent with less than an hour of training. As demonstrated, our own group members also played the game using the keyboard agent, although none of the group members can perform more than \(20\) in any of the trials. In this context, the trained agent outperforms every group member with the score of \(28\). Therefore, the answer to our initial question is yes! The trained agent with the specified model can outperform both random and human agents.

Throughout the project, we learned the basic approaches of reinforcement learning. We learned that by using CNNs and DQNs, it is possible to learn control policies directly from high-dimensional sensory input using reinforcement learning. The resultant agent is satisfactory to respond our question with ``yes''. In addition to basic theoretical approach, we also had the chance to interact with a practical issue that we were not expecting to face. While we were trying to adjust the exploration versus exploitation hyperparameter, \(\varepsilon \), we faced with an issue that we later learned it is called the credit assignment problem. The credit assignment problem means that it can happen that we choose an action, and we only win or lose hundreds of actions later, leaving us with no idea of as to which of our actions led to this win or lose, thus, making it difficult to learn from our actions~\autocite{mnih2013playing}. We learned that solely increasing the training duration does increase the performance of the agent. We learned that in order to overcome this credit assignment pitfall, using random restarts actually increases the performance of the agent, by adjusting the exploration versus exploitation problem in the direction of exploration.

As the future work of the project, performance of the agent can be increased by changing the deep neural network architecture and increasing the training duration. During our project, we have encountered with more complex approaches like double DQNs, Never Give-Up (NGU), etc. Experimentation on these approaches can be expressed as potential future work. Furthermore, these approaches can be generalized as \textit{DeepMind}'s \texttt{Agent57} to provide trained agents for \(57\) atari games~\autocite{agent57}.
