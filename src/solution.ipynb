{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0d9acccaf128a40f80b71cb10f1593ad35e77161b12a7bbe9b47f3e6ab16a1347",
   "display_name": "Python 3.8.8 64-bit ('dqn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Standard Library\n",
    "from pathlib import Path\n",
    "from collections import deque, namedtuple\n",
    "from typing import List\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "print(f'PyTorch using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoFire(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self._lives = 5\n",
    "        self.action_space = Discrete(3)\n",
    "        self._fire = False\n",
    "\n",
    "    def step(self, action):\n",
    "        if self._fire:\n",
    "            obs, reward, done, info = self.env.step(1)\n",
    "            self._fire = False\n",
    "        elif action > 0:\n",
    "            obs, reward, done, info = self.env.step(action + 1)\n",
    "        else:\n",
    "            obs, reward, done, info = self.env.step(action + 1)\n",
    "\n",
    "        if info['ale.lives'] < self._lives:\n",
    "            self._lives = info['ale.lives']\n",
    "            self._fire = True\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epsilon:\n",
    "\n",
    "    def __init__(self, start, end, decay):\n",
    "        self._start = start\n",
    "        self._end = end\n",
    "        self._decay = decay\n",
    "        self._current = start\n",
    "\n",
    "    def reset(self):\n",
    "        self._current = self._start\n",
    "    \n",
    "    def __next__(self):\n",
    "        self._current = max(self._current * self._decay, self._end)\n",
    "        return self._current\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._current\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        return self._start\n",
    "\n",
    "    @property\n",
    "    def decay(self):\n",
    "        return self._decay\n",
    "\n",
    "    @property\n",
    "    def end(self):\n",
    "        return self._end\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Epsilon(start={self.start}, end={self.end}, decay={self.decay}, current={self.value})'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'next_state', 'reward', 'done'])\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.memory.append(Transition(state, action, next_state, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_count):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_count = action_count\n",
    "        self.net = DQN.construct_net(state_shape[0], action_count)\n",
    "\n",
    "    @classmethod\n",
    "    def construct_net(cls, channels, actions):\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions),\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def act(self, X):\n",
    "        return torch.argmax(self.net(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        dt = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "        save_dir = Path.joinpath(Path.cwd(), save_dir, dt)\n",
    "        if not save_dir.exists():\n",
    "            save_dir.mkdir(parents=True)\n",
    "\n",
    "        self.save_log = save_dir / \"log.csv\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write('Episode,Step,Mean Reward,Mean Length, Mean Q Value, Time Delta, Time\\n')\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(f'{episode},'\n",
    "                f'{step},'\n",
    "                f'{epsilon:0.4f},'\n",
    "                f'{mean_ep_reward:0.3f},'\n",
    "                f'{mean_ep_length:0.3f},'\n",
    "                f'{mean_ep_loss:0.3f},'\n",
    "                f'{mean_ep_q:0.3f},'\n",
    "                f'{time_since_last_record:0.3f},'\n",
    "                f'{datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")}\\n'\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make('Breakout-v0')\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    env = AutoFire(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EVERY = 1e4\n",
    "BURNIN = 1e4\n",
    "LEARN_EVERY = 3\n",
    "SYNC_EVERY = 100\n",
    "SAVE_DIR = 'models'\n",
    "\n",
    "hyper = {\n",
    "    'REPLAY_MEMORY_SIZE': 12000,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'GAMMA': 0.9,\n",
    "    'LEARNING_RATE': 1e-3, \n",
    "    'MOMENTUM': 0.95,\n",
    "    'EPSILON_START': 1,\n",
    "    'EPSILON_END': 0.2,\n",
    "    'EPSILON_DECAY': 0.999995,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_shape, action_count, device, *, path=None, epsilon=True, **hyper):\n",
    "        self.device = device\n",
    "        self.action_count = action_count\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        # Construct target, and online net\n",
    "        if path is None:\n",
    "            self._target = DQN(state_shape, action_count).to(device)\n",
    "        else:\n",
    "            self._target = torch.load(path).to(device)\n",
    "        \n",
    "        if epsilon:\n",
    "            self.epsilon = Epsilon(start=hyper['EPSILON_START'],\n",
    "                end=hyper['EPSILON_END'],\n",
    "                decay=hyper['EPSILON_DECAY'])\n",
    "        else:\n",
    "            self.epsilon = Epsilon(hyper['EPSILON_END'], hyper['EPSILON_END'], 1)\n",
    "        \n",
    "        self._online = copy.deepcopy(self._target).to(device)\n",
    "        for p in self._online.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self._target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.step = 0\n",
    "        self.memory = ReplayMemory(hyper['REPLAY_MEMORY_SIZE'])\n",
    "        self.batch_size = hyper['BATCH_SIZE']\n",
    "        self.gamma = hyper['GAMMA']\n",
    "        self.optimizer = torch.optim.Adam(self._online.parameters(), lr=hyper['LEARNING_RATE'])\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def save(self):\n",
    "        filename = f\"dqn{int(self.step // SAVE_EVERY)}.chkpt\"\n",
    "        save_path = Path.joinpath(Path.cwd(), SAVE_DIR, filename)\n",
    "        torch.save(self._target, save_path)\n",
    "        print(f'DQN saved to {save_path} at step {self.step}')\n",
    "    \n",
    "    def reset_epsilon(self):\n",
    "        self.epsilon.reset()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.step % SYNC_EVERY == 0:\n",
    "            self.sync_Q_target()\n",
    "        if self.step % SAVE_EVERY == 0:\n",
    "            self.save()\n",
    "        if self.step < BURNIN:\n",
    "            return None, None\n",
    "        if self.step % LEARN_EVERY != 0:\n",
    "            return None, None\n",
    "\n",
    "        state, action, next_state, reward, done = self.recall()\n",
    "        td_est = self.td_estimate(state, action)\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "        return td_est.mean().item(), loss\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self._target.load_state_dict(self._online.state_dict())\n",
    "    \n",
    "    def td_estimate(self, state, action):\n",
    "        action = action.view(-1)\n",
    "        current_Q = self.online(state)\n",
    "        return current_Q[np.arange(0, self.batch_size), action]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.online(next_state)\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.target(next_state)[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "    def remember(self, state, action, next_state, reward, done):\n",
    "        state = torch.tensor(state.__array__()).to(self.device)\n",
    "        action = torch.tensor([action]).to(self.device)\n",
    "        next_state = torch.tensor(next_state.__array__()).to(self.device)\n",
    "        reward = torch.tensor([reward]).to(self.device)\n",
    "        done = torch.tensor([done]).to(self.device)\n",
    "\n",
    "        self.memory.push(state, action, next_state, reward, done)\n",
    "    \n",
    "    def recall(self):\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def target(self, X):\n",
    "        return self._target(X)\n",
    "\n",
    "    def online(self, X):\n",
    "        return self._online(X)\n",
    "    \n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "        '''\n",
    "        self.step += 1\n",
    "        if np.random.rand() < next(self.epsilon):\n",
    "            return self.explore()\n",
    "        else:\n",
    "            return self.exploit(state)\n",
    "\n",
    "    def explore(self):\n",
    "        return np.random.randint(self.action_count)\n",
    "\n",
    "    def exploit(self, state):\n",
    "        state = torch.tensor(state.__array__()).to(self.device).view(-1, *self.state_shape)\n",
    "        return self._target.act(state).item()\n",
    "\n",
    "    @property\n",
    "    def exploration_rate(self):\n",
    "        return self.epsilon.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "ENV_SHAPE = env.observation_space.shape\n",
    "ACTION_COUNT = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(ENV_SHAPE, ACTION_COUNT, device, path='models/dqn1.chkpt', **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "save_dir = Path(SAVE_DIR)\n",
    "episodes = 4000\n",
    "\n",
    "for i in itertools.count():\n",
    "    agent.reset_epsilon()\n",
    "    logger = MetricLogger(save_dir)\n",
    "    print(f'Running Epoch {i+1}')\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, next_state, reward, done)\n",
    "            q, loss = agent.learn()\n",
    "            logger.log_step(reward, loss, q)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        logger.log_episode()\n",
    "        if e % 20 == 0:\n",
    "            logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.step)"
   ]
  }
 ]
}